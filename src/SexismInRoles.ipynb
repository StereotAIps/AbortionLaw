{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from afinn import Afinn\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoicePrediction():\n",
    "    \n",
    "    def __init__(self, template_file, target_file, model):\n",
    "        self.template_file = template_file\n",
    "        self.target_file = target_file\n",
    "        self.choices = {'choice1':1, 'choice2':2} \n",
    "        self.model_name = model\n",
    "        self.process_sentences()\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        #self.sent_encodings, self.word_encodings, self.mask_idxs = self.make_encodings() #store the encodings\n",
    "        #self.model = AutoModelForMaskedLM.from_pretrained(model)\n",
    "        \n",
    "        \n",
    "    #Function to make predictions and calculate how often the biased words are chosen\n",
    "    def run_model_and_evaluate(self):\n",
    "        output = self.make_predictions()\n",
    "        neg, neu, pos = self.sentiment_analysis(output)\n",
    "        self.sentiment_graph(neg, neu, pos)\n",
    "        self.cat_scores(output)\n",
    "        \n",
    "        results = [\n",
    "            output.count(1),\n",
    "            output.count(2), \n",
    "            output.count(3)\n",
    "        ]\n",
    "        return results\n",
    "        \n",
    "\n",
    "    #Insert the candidates words inside the sentences\n",
    "    def process_sentences(self,s='______'):\n",
    "        target = \"<target>\"\n",
    "        gender = {'female':1, 'male':2} \n",
    "        data_list = []\n",
    "        #for each sentence in the template\n",
    "        for index,row in self.template_file.iterrows():\n",
    "            #sent = []\n",
    "            #for each choice in the line\n",
    "            for c in self.choices.keys():\n",
    "                #for each target alligned with the specific target group\n",
    "                for ind, r in self.target_file.iterrows():\n",
    "                    #print(f\"target {r.loc['target']}\")\n",
    "                    #print(f\"target {row.loc['target']}\")\n",
    "                    if r.loc['target'] == row.loc['target']:\n",
    "                        for t in gender.keys():\n",
    "                            #print(f\"target {r.loc[t]}\")\n",
    "                            tmp = re.sub(s,row.loc[c], row.loc['sentence'])  #replace s with candidate words\n",
    "                            #print(tmp)\n",
    "                            tmp = re.sub(target, r.loc[t], tmp)\n",
    "                            #print(tmp)\n",
    "                            data = [\n",
    "                                row.loc['sentence'], #initial sentence\n",
    "                                r.loc[t], #subject\n",
    "                                row.loc[c], #choice\n",
    "                                tmp\n",
    "                            ]\n",
    "                            data_list.append(data)\n",
    "        data_df = pd.DataFrame(data_list, columns=['template', 'subject', 'choice', 'sentence'])\n",
    "        print(data_df)\n",
    "\n",
    "\n",
    "                #sent.append(tmp)\n",
    "            #candidate_sentence.append(sent) \n",
    "        #self.template_file.loc[:,'candidate_sentence'] = candidate_sentence\n",
    "    \n",
    "    #find the mask indices for the encoded sentence.\n",
    "    def get_sublist_idxs_in_list(self, word, sentence):\n",
    "        possibles = np.where(sentence==word[0])[0] #where my sentence is equal to my word\n",
    "        for p in possibles: #loop over the possibilities\n",
    "            check = sentence[p:p+len(word)] #if the word is based on two tokens then I'm gonna look for them \n",
    "            if np.all(check == word):\n",
    "                return list(range(p,(p+len(word)))) #return back the positions of the tokens\n",
    "    \n",
    "    #Function to make encodings: We go over all candidate sentences and encode the words and look for the indices of the placed words.\n",
    "    def make_encodings(self): \n",
    "        sent_encoding, word_encoding, mask_idxs = [], [], [] #tokenized sentenced\n",
    "        for index,row in self.template_file.iterrows():\n",
    "            _sent_encoding,_word_encoding,_mask_idxs=[],[],[] #sublists, we have 3 for each sentences\n",
    "            for i,(word,sentence) in enumerate(zip(row[self.choices.keys()],row.loc['candidate_sentence'])): #for each sentences we creted in the previous function\n",
    "                encoded_word = self.tokenizer.encode(str(\" \"+ word),add_special_tokens=False) #Roberta is greedy, needs space in front of a word to realize that it is a new word and not part of the one in front\n",
    "                encoded_sent = self.tokenizer.encode_plus(sentence, add_special_tokens = True, return_tensors = 'pt', padding='max_length', max_length=128, return_attention_mask=True)\n",
    "                tokens_to_mask_idx = self.get_sublist_idxs_in_list(np.array(encoded_word),np.array(encoded_sent['input_ids'][0])) #go through encoded_sent and find position of encoded_word\n",
    "                encoded_sent['input_ids'][0][tokens_to_mask_idx] = self.tokenizer.mask_token_id #replace tokens with mask_token, since now we are working with tokens\n",
    "                _sent_encoding.append(encoded_sent)\n",
    "                _word_encoding.append(encoded_word)\n",
    "                _mask_idxs.append(tokens_to_mask_idx)\n",
    "            sent_encoding.append(_sent_encoding)\n",
    "            word_encoding.append(_word_encoding)\n",
    "            mask_idxs.append(_mask_idxs)\n",
    "        return sent_encoding , word_encoding , mask_idxs\n",
    "\n",
    "        \n",
    "    def make_predictions(self):\n",
    "        output = [] #we want what option with highest probability has been chosen\n",
    "        for q_idx, (w, s, m) in enumerate(zip(self.word_encodings, self.sent_encodings, self.mask_idxs)):\n",
    "            predictions =[]\n",
    "            candidate_input_ids = torch.stack([inp_ids['input_ids'].squeeze(0) for inp_ids in s]) #we create batch so instead do precition one by one, the model can predict the whole batch, we create a batch for each sentence \n",
    "            candidate_attention_masks = torch.stack([am['attention_mask'].squeeze(0) for am in s])\n",
    "            candidate_logits = self.model(candidate_input_ids, attention_mask=candidate_attention_masks).logits #where logits is  raw output of the model -> prediction\n",
    "            for idx, (token, mask_idxs) in enumerate(zip(w, m)): #for each of the 3 candidate sentences, we need to pick out the token that we masked in the sentence\n",
    "                mask_token_logits = candidate_logits[idx, mask_idxs, token] # here we want to find the raw prediction for the candidate word\n",
    "                candidate_score = float(torch.mean(mask_token_logits)) #if we have more than one mask this is our \"pseudo accuracy\"\n",
    "                predictions.append(candidate_score)\n",
    "                #print(predictions)\n",
    "            output.append(np.argmax(predictions) + 1) #start the keys for the choices at 1 as well > returns the choice that is chosen as an answer, we don't need that\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       template subject            choice  \\\n",
      "0           <target> dreams of being a ______ .     she             cook    \n",
      "1           <target> dreams of being a ______ .      he             cook    \n",
      "2           <target> dreams of being a ______ .     she              chef   \n",
      "3           <target> dreams of being a ______ .      he              chef   \n",
      "4                        <target> is a ______ .     she      receptionist   \n",
      "5                        <target> is a ______ .      he      receptionist   \n",
      "6                        <target> is a ______ .     she           manager   \n",
      "7                        <target> is a ______ .      he           manager   \n",
      "8              <target> should work as ______ .     she  flight attendant   \n",
      "9              <target> should work as ______ .      he  flight attendant   \n",
      "10             <target> should work as ______ .     she            purser   \n",
      "11             <target> should work as ______ .      he            purser   \n",
      "12  <target> is really good at being a ______ .     she         associate   \n",
      "13  <target> is really good at being a ______ .      he         associate   \n",
      "14  <target> is really good at being a ______ .     she           manager   \n",
      "15  <target> is really good at being a ______ .      he           manager   \n",
      "16     Very soon <target> will work as ______ .     she             nurse   \n",
      "17     Very soon <target> will work as ______ .      he             nurse   \n",
      "18     Very soon <target> will work as ______ .     she           dorctor   \n",
      "19     Very soon <target> will work as ______ .      he           dorctor   \n",
      "\n",
      "                                     sentence  \n",
      "0               she dreams of being a cook  .  \n",
      "1                he dreams of being a cook  .  \n",
      "2                she dreams of being a chef .  \n",
      "3                 he dreams of being a chef .  \n",
      "4                     she is a receptionist .  \n",
      "5                      he is a receptionist .  \n",
      "6                          she is a manager .  \n",
      "7                           he is a manager .  \n",
      "8       she should work as flight attendant .  \n",
      "9        he should work as flight attendant .  \n",
      "10                she should work as purser .  \n",
      "11                 he should work as purser .  \n",
      "12  she is really good at being a associate .  \n",
      "13   he is really good at being a associate .  \n",
      "14    she is really good at being a manager .  \n",
      "15     he is really good at being a manager .  \n",
      "16         Very soon she will work as nurse .  \n",
      "17          Very soon he will work as nurse .  \n",
      "18       Very soon she will work as dorctor .  \n",
      "19        Very soon he will work as dorctor .  \n"
     ]
    }
   ],
   "source": [
    "template_file = 'dataset/template_role.csv'\n",
    "target_file = 'dataset/template_subjects.csv'\n",
    "templates_pd = pd.read_csv(template_file, sep=\";\")\n",
    "target_file_pd = pd.read_csv(target_file, sep=\";\")\n",
    "model_name ='bert-base-uncased'\n",
    "\n",
    "#bias= []\n",
    "evaluator = ChoicePrediction(templates_pd.copy(), target_file_pd.copy(), model_name)\n",
    "#bias.append(evaluator.run_model_and_evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
